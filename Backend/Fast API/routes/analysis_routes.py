# routes/analysis_routes.py (Ù…Ø­Ø³Ù‘Ù† - Ù…Ø¹ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ø°ÙƒÙŠØ© + Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØªÙŠÙ†)

from fastapi import APIRouter, Depends, HTTPException
from pydantic import BaseModel, Field
from typing import List, Optional, Dict
from config.database_config import db
from services.rag_service import rag_service
from services.gemini_service import gemini_service 
from services.embedding_service import embedding_service
from services.supervisor_recommendation import supervisor_recommendation
from utils.prompts import (
    create_initial_analysis_prompt, 
    create_extended_analysis_prompt,
    extract_json_safely
)
from routes.language_detector import detect_combined_language
from bson import ObjectId
import datetime
from datetime import UTC 

import json 
import traceback

router = APIRouter()

students_collection = db["users"]
ideas_collection = db["IdeaAnalysis"]

# ============================================================================
# ğŸ“š Models
# ============================================================================
class AnalysisRequest(BaseModel):
    title: str = Field(..., min_length=5, max_length=200)
    description: str = Field(..., min_length=20, max_length=2000)
    technologies: Optional[List[str]] = Field(default=None)
    student_id: Optional[str] = Field(default=None)
    email: Optional[str] = Field(default=None)
    language: str = Field(default="en")

class AnalysisResult(BaseModel):
    id: str
    message: str
    stage_1_initial_analysis: dict
    stage_2_extended_analysis: dict
    similar_projects: list
    recommended_supervisors: list

# ============================================================================
# ğŸ› ï¸ Helper Functions
# ============================================================================

def fix_id(doc):
    if doc and "_id" in doc:
        doc["_id"] = str(doc["_id"])
    return doc

def clean_objectid_recursive(obj):
    if isinstance(obj, dict):
        return {k: clean_objectid_recursive(v) for k, v in obj.items()}
    elif isinstance(obj, list):
        return [clean_objectid_recursive(item) for item in obj]
    elif isinstance(obj, ObjectId):
        return str(obj)
    else:
        return obj

# ğŸ†• Ø¯Ø§Ù„Ø© Ø¬Ø¯ÙŠØ¯Ø©: Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª ÙˆØªØµÙ†ÙŠÙÙ‡Ø§
def compare_skills(required_skills: List[str], student_skills: List[str]) -> Dict:
    """
    Ù…Ù‚Ø§Ø±Ù†Ø© Ø°ÙƒÙŠØ© Ø¨ÙŠÙ† Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© ÙˆÙ…Ù‡Ø§Ø±Ø§Øª Ø§Ù„Ø·Ø§Ù„Ø¨
    
    Returns:
        {
            "matched": [...],  # Ù…Ù‡Ø§Ø±Ø§Øª Ù…ÙˆØ¬ÙˆØ¯Ø© (Ø£Ø®Ø¶Ø±)
            "gaps": [...],     # Ù…Ù‡Ø§Ø±Ø§Øª Ù†Ø§Ù‚ØµØ© (Ø£Ø­Ù…Ø±)
            "match_percentage": 75.0
        }
    """
    if not required_skills:
        return {"matched": [], "gaps": [], "match_percentage": 0.0}
    
    if not student_skills:
        return {
            "matched": [], 
            "gaps": required_skills, 
            "match_percentage": 0.0
        }
    
    # ØªØ­ÙˆÙŠÙ„ Ø¥Ù„Ù‰ lowercase Ù„Ù„Ù…Ù‚Ø§Ø±Ù†Ø©
    student_skills_lower = [s.lower().strip() for s in student_skills]
    required_skills_lower = [s.lower().strip() for s in required_skills]
    
    matched = []
    gaps = []
    
    # Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© Ù…Ø·Ø§Ø¨Ù‚Ø© Ø°ÙƒÙŠØ© (ØªØ¯Ø¹Ù… Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª ÙˆØ§Ù„Ø§Ø®ØªØµØ§Ø±Ø§Øª)
    synonyms = {
        "python": ["python3", "py"],
        "javascript": ["js", "node", "nodejs", "node.js"],
        "react": ["reactjs", "react.js"],
        "machine learning": ["ml", "deep learning", "dl"],
        "artificial intelligence": ["ai", "ml"],
        "database": ["sql", "mysql", "postgresql", "mongodb"],
        "git": ["github", "version control"],
        "docker": ["containerization", "containers"],
        "api": ["rest api", "restful", "rest"],
    }
    
    for req_skill in required_skills:
        req_lower = req_skill.lower().strip()
        found = False
        
        # 1. Ù…Ø·Ø§Ø¨Ù‚Ø© Ù…Ø¨Ø§Ø´Ø±Ø©
        if req_lower in student_skills_lower:
            matched.append(req_skill)
            found = True
            continue
        
        # 2. Ù…Ø·Ø§Ø¨Ù‚Ø© Ø¬Ø²Ø¦ÙŠØ© (substring)
        for student_skill in student_skills_lower:
            if req_lower in student_skill or student_skill in req_lower:
                matched.append(req_skill)
                found = True
                break
        
        if found:
            continue
        
        # 3. Ù…Ø·Ø§Ø¨Ù‚Ø© Ø§Ù„Ù…Ø±Ø§Ø¯ÙØ§Øª
        for key, values in synonyms.items():
            if req_lower == key or req_lower in values:
                for student_skill in student_skills_lower:
                    if student_skill == key or student_skill in values:
                        matched.append(req_skill)
                        found = True
                        break
            if found:
                break
        
        # Ø¥Ø°Ø§ Ù„Ù… ØªÙˆØ¬Ø¯ Ù…Ø·Ø§Ø¨Ù‚Ø©ØŒ ØªÙØ¶Ø§Ù Ù„Ù„ÙØ¬ÙˆØ§Øª
        if not found:
            gaps.append(req_skill)
    
    # Ø­Ø³Ø§Ø¨ Ù†Ø³Ø¨Ø© Ø§Ù„ØªØ·Ø§Ø¨Ù‚
    match_percentage = (len(matched) / len(required_skills)) * 100 if required_skills else 0.0
    
    return {
        "matched": matched,
        "gaps": gaps,
        "match_percentage": round(match_percentage, 1)
    }

def create_fallback_extended_analysis(
    initial_analysis: dict, 
    supervisors: list, 
    similar_projects: list
) -> dict:
    """Fallback Ù…Ø­Ø³Ù‘Ù† Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª"""
    project_title = initial_analysis.get('Project_Title', 'the project')
    technical_domain = initial_analysis.get('Domain', {}).get('Technical_Domain', 'related technologies')
    general_domain = initial_analysis.get('Domain', {}).get('General_Domain', 'the field')
    
    formatted_supervisors = []
    for i, s in enumerate(supervisors[:5]):
        name = s.get("Name") or s.get("name", "Unknown Supervisor")
        dept = s.get("Department") or s.get("department", "N/A")
        email = s.get("Email") or s.get("email", "N/A")
        papers_count = len(s.get("recent_papers", []))
        
        justification = (
            f"{s.get('justification', '') or s.get('explanation', '') or f'Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±Ù Ù…Ù† Ù‚Ø³Ù… {dept} ÙŠÙ…ØªÙ„Ùƒ Ø®Ø¨Ø±Ø© ÙÙŠ {technical_domain} ÙˆÙ„Ø¯ÙŠÙ‡ {papers_count} Ø¨Ø­Ø« Ø­Ø¯ÙŠØ« Ù…Ù…Ø§ ÙŠØ¬Ø¹Ù„Ù‡ Ù…Ù†Ø§Ø³Ø¨Ø§Ù‹ Ù„Ø¥Ø´Ø±Ø§Ù {project_title}.'}"
        )
        
        formatted_supervisors.append({
            "Name": name,
            "Department": dept,
            "Email": email,
            "Justification": justification
        })
    
    formatted_projects = []
    for p in similar_projects[:5]:
        title = p.get("title") or p.get("project_title") or p.get("projrct_title", "Related Project")
        year = p.get("year", "N/A")
        dept = p.get("department", "N/A")
        domain = p.get("project_domain", general_domain)
        
        relevance = (
            f"Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ù…Ù† Ø¹Ø§Ù… {year} ÙˆÙ‚Ø³Ù… {dept} ÙŠØ³ØªÙƒØ´Ù ØªØ­Ø¯ÙŠØ§Øª Ù…Ù…Ø§Ø«Ù„Ø© ÙÙŠ Ù…Ø¬Ø§Ù„ {domain}ØŒ ÙˆÙŠÙˆÙØ± Ù†Ø¸Ø±Ø© Ù‚ÙŠÙ…Ø© Ù„ÙÙƒØ±Ø© {project_title}."
        )
        
        formatted_projects.append({
            "Title": title,
            "Year": year,
            "Department": dept,
            "Relevance": relevance
        })
    
    improvements = [
        f"ÙŠØ¬Ø¨ ØªØ­Ø³ÙŠÙ† ØµÙŠØ§ØºØ© Ø§Ù„Ù…Ø´ÙƒÙ„Ø© Ù„ØªÙƒÙˆÙ† Ø£ÙƒØ«Ø± ØªØ±ÙƒÙŠØ²Ø§Ù‹ Ø¹Ù„Ù‰ Ø¬Ø§Ù†Ø¨ {technical_domain}.",
        f"Ø§Ù‚ØªØ±Ø­ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ù…Ø§Ø±Ø³Ø§Øª {technical_domain} Ø§Ù„Ø­Ø¯ÙŠØ«Ø© Ù„ØªØ¹Ø²ÙŠØ² Ù‚Ø§Ø¨Ù„ÙŠØ© Ø§Ù„ØªÙˆØ³Ø¹.",
        f"ÙŠØ¬Ø¨ ØªØ·Ø¨ÙŠÙ‚ Ø§Ø®ØªØ¨Ø§Ø±Ø§Øª Ø£Ø¯Ø§Ø¡ Ù‚ÙˆÙŠØ© Ù„Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙƒÙØ§Ø¡Ø© {project_title}.",
    ]
    
    executive_summary = initial_analysis.get('Executive_Summary', '')
    
    final_summary = (
        f"{executive_summary} "
        f"ÙŠÙ‡Ø¯Ù Ù‡Ø°Ø§ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ Ø¥Ù„Ù‰ Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª ÙÙŠ Ù…Ø¬Ø§Ù„ {general_domain} Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ØªÙ‚Ù†ÙŠØ§Øª {technical_domain} Ø§Ù„Ù…Ø°ÙƒÙˆØ±Ø©."
    )
    
    return {
        "Supervisors": formatted_supervisors,
        "Similar_Projects": formatted_projects,
        "Improvements": improvements,
        "Final_Proposal": {
            "Summary": final_summary
        }
    }

# ============================================================================
# ğŸ¯ MAIN ENDPOINT - POST /analyze (Ù…Ø­Ø³Ù‘Ù† + Ø¯Ø¹Ù… Ø§Ù„Ù„ØºØªÙŠÙ†)
# ============================================================================

@router.post("/analyze")
async def analyze_idea(request: AnalysisRequest):
    """
    ğŸ” ØªØ­Ù„ÙŠÙ„ ÙÙƒØ±Ø© Ù…Ø´Ø±ÙˆØ¹ Ø§Ù„ØªØ®Ø±Ø¬ Ù…Ø¹ Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª ÙˆØ¯Ø¹Ù… Ø§Ù„Ù„ØºØªÙŠÙ†
    """
    try:
        print(f"\n{'='*60}")
        print(f"ğŸ”¥ Analysis Request:")
        print(f"   Title: {request.title}")
        print(f"   Student ID: {request.student_id}")
        print(f"   Email: {request.email}")
        print(f"{'='*60}\n")
        
        # ========== Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø·Ø§Ù„Ø¨ ÙˆØ¬Ù„Ø¨ Ù…Ù‡Ø§Ø±Ø§ØªÙ‡ ==========
        student = {}
        student_skills = []
        
        if request.student_id or request.email:
            try:
                query = {}
                if request.student_id:
                    if ObjectId.is_valid(request.student_id):
                        query["_id"] = ObjectId(request.student_id)
                    else:
                        raise ValueError("Invalid student_id format")
                elif request.email:
                    query["email"] = request.email
                
                student = students_collection.find_one(query)
                if student:
                    print(f"âœ… Student found: {student.get('full_name', 'N/A')}")
                    student = clean_objectid_recursive(student)
                    
                    # ğŸ†• Ø¬Ù„Ø¨ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª Ù…Ù† Ø§Ù„Ù…Ù„Ù Ø§Ù„Ø´Ø®ØµÙŠ
                    student_skills = student.get("skills", [])
                    print(f"ğŸ“Š Student Skills ({len(student_skills)}): {', '.join(student_skills[:5])}...")
                else:
                    print(f"âš ï¸ Student not found")
                    student = {}
            except Exception as e:
                print(f"âš ï¸ Error loading student: {str(e)}")
                student = {}
        
        # ========== ğŸ†• Ø§Ù„ÙƒØ´Ù Ø§Ù„ØªÙ„Ù‚Ø§Ø¦ÙŠ Ø¹Ù† Ø§Ù„Ù„ØºØ© ==========
        detected_language = detect_combined_language(request.title, request.description)
        print(f"ğŸŒ Detected Language: {detected_language.upper()}")
        print(f"   {'Arabic' if detected_language == 'ar' else 'English'} content detected\n")
        
        # ØªÙ†Ø¸ÙŠÙ… Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        idea_dict = {
            "title": request.title,
            "description": request.description,
            "technologies": request.technologies or [],
            "student_id": request.student_id if request.student_id else None,
            "email": request.email if request.email else None,
            "created_at": datetime.datetime.now(UTC)
        }
        
        # ========== Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1: Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø¨Ø¯Ø¦ÙŠ (Ù…Ø¹ Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª + Ø§Ù„Ù„ØºØ©) ==========
        print("ğŸ”µ Stage 1: Initial Analysis (with Skills Comparison)...")
        
        initial_prompt = create_initial_analysis_prompt(
            idea=idea_dict, 
            student=student,
            language=detected_language  # ğŸ†• ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©
        )
        initial_analysis_text = await gemini_service.analyze_idea(initial_prompt)
        
        try:
            initial_analysis = extract_json_safely(initial_analysis_text)
            print("âœ… Stage 1: Success\n")
        except (ValueError, Exception) as e:
            print(f"âš ï¸ Stage 1 JSON Error: {str(e)}")
            print("ğŸ”„ Retrying with simplified prompt...\n")
            
            simple_prompt = f"""Output ONLY valid JSON for project analysis.
Project: {request.title}
Description: {request.description}
Student Skills: {', '.join(student.get('skills', []))}
Required JSON:
{{
  "Project_Title": "title",
  "Executive_Summary": "100-150 words",
  "Domain": {{"General_Domain": "domain", "Technical_Domain": "technical"}},
  "Required_Skills": {{"Skills": [], "Matches": [], "Gaps": []}},
  "SWOT_Analysis": {{"Strengths": [], "Weaknesses": [], "Opportunities": [], "Threats": []}},
  "Target_Audience": {{"Primary": [], "Secondary": []}}
}}
JSON only:"""
            
            retry_text = await gemini_service.analyze_idea(simple_prompt)
            initial_analysis = extract_json_safely(retry_text)
            print("âœ… Stage 1: Success (after retry)\n")
        
        # ğŸ†• Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ù…Ù‡Ø§Ø±Ø§Øª ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù…Ø­Ø³Ù‘Ù†Ø©
        required_skills_obj = initial_analysis.get("Required_Skills", {})
        required_skills_list = required_skills_obj.get("Skills", [])
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø°ÙƒÙŠØ©
        skills_comparison = compare_skills(required_skills_list, student_skills)
        
        # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø© Ø§Ù„Ø°ÙƒÙŠØ©
        initial_analysis["Required_Skills"]["Matches"] = skills_comparison["matched"]
        initial_analysis["Required_Skills"]["Gaps"] = skills_comparison["gaps"]
        initial_analysis["Required_Skills"]["Match_Percentage"] = skills_comparison["match_percentage"]
        
        print(f"ğŸ“Š Skills Analysis:")
        print(f"   âœ… Matched: {len(skills_comparison['matched'])}")
        print(f"   âŒ Gaps: {len(skills_comparison['gaps'])}")
        print(f"   ğŸ“ˆ Match Rate: {skills_comparison['match_percentage']}%\n")
        
        # ========== Ø§Ù„ØªØ­Ø¶ÙŠØ± Ù„Ù„Ù…Ø±Ø­Ù„Ø© 2 ==========
        print("ğŸ”µ Preparing Stage 2: Finding similar projects & supervisors...")
        
        idea_text = f"{request.title} {request.description}"
        similarity_report = await rag_service.find_similar_projects(idea_text, top_k=5)
        
        similar_projects_raw = similarity_report.get("reranked_projects", [])
        similar_projects = [clean_objectid_recursive(proj) for proj in similar_projects_raw]
        
        print(f"\n--- ğŸ” ØªÙ‚Ø±ÙŠØ± ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙƒØ±Ø§Ø± ---")
        print(f"   - Ø­Ø§Ù„Ø© Ø§Ù„ØªÙƒØ±Ø§Ø±: {similarity_report.get('duplication_status', 'N/A')}")
        print(f"   - Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©: {len(similar_projects)}")
        
        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„ÙƒÙ„Ù…Ø§Øª Ø§Ù„Ù…ÙØªØ§Ø­ÙŠØ©
        technical_keywords = []
        if "Domain" in initial_analysis:
            domain = initial_analysis["Domain"]
            if isinstance(domain, dict):
                general = domain.get("General_Domain", "")
                technical = domain.get("Technical_Domain", "")
                if general:
                    technical_keywords.extend([w.strip() for w in general.split(",")[:2]])
                if technical:
                    parts = technical.split(",")
                    for part in parts[:3]:
                        technical_keywords.extend(part.strip().split()[:2])
        
        all_keywords = list(set([
            k.strip().lower() 
            for k in (request.technologies or []) + technical_keywords 
            if k and len(k.strip()) > 2
        ]))
        
        search_text = " ".join(all_keywords[:20])
        print(f"ğŸ” Search keywords: {search_text[:150]}...\n")
        
        # ØªÙˆØµÙŠØ© Ø§Ù„Ù…Ø´Ø±ÙÙŠÙ†
        recommended_supervisors_data = await supervisor_recommendation.recommend_supervisors(
            idea_text=idea_text,
            student_major=student.get("major", "Computer Science"),
            top_k=15
        )
        
        supervisors_for_prompt = []
        for s in recommended_supervisors_data:
            supervisor_clean = clean_objectid_recursive(s["supervisor"])
            supervisor_clean["reranked_score"] = s.get("final_score", 0.0)
            supervisor_clean["justification"] = s.get("justification", "No specific explanation.")
            
            recent_papers_clean = clean_objectid_recursive(s.get("recent_papers", []))
            supervisor_clean["recent_papers"] = recent_papers_clean
            supervisors_for_prompt.append(supervisor_clean)
        
        # ========== Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2: Ø§Ù„ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠ (Ù…Ø¹ Ø§Ù„Ù„ØºØ©) ==========
        print("ğŸ”µ Stage 2: Extended Analysis & Report...")
        
        extended_prompt = create_extended_analysis_prompt(
            initial_analysis=initial_analysis,
            student=student,
            supervisors=supervisors_for_prompt,
            similar_projects=similar_projects,
            language=detected_language  # ğŸ†• ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©
        )
        
        extended_analysis_text = await gemini_service.analyze_idea(extended_prompt)
        
        try:
            extended_analysis = extract_json_safely(extended_analysis_text)
            print("âœ… Stage 2: Success\n")
        except (ValueError, Exception) as e:
            print(f"âš ï¸ Stage 2 JSON Error: {str(e)}")
            print("ğŸ”„ Using enhanced fallback analysis...\n")
            extended_analysis = create_fallback_extended_analysis(
                initial_analysis,
                supervisors_for_prompt, 
                similar_projects
            )
            print("âœ… Stage 2: Fallback completed\n")
        
        # ========== Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ ==========
        print("ğŸ’¾ Saving results to MongoDB...")
        
        idea_data = idea_dict.copy()
        idea_data["initial_analysis"] = initial_analysis
        idea_data["extended_analysis"] = extended_analysis
        idea_data["duplication_status"] = similarity_report.get("duplication_status", "Not Analyzed")
        idea_data["duplication_report"] = similarity_report.get("analysis_report", "No RAG report available.")
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø´Ø§Ø±ÙŠØ¹ Ø§Ù„Ù…Ø´Ø§Ø¨Ù‡Ø©
        idea_data["similar_projects"] = [
            {
                "_id": str(p.get("id", "") or p.get("_id", "")),
                "title": p.get("title") or p.get("project_title", "Ø¨Ø¯ÙˆÙ† Ø¹Ù†ÙˆØ§Ù†"),
                "department": p.get("department", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "year": p.get("year", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "abstract": (p.get("abstract") or "")[:200],
                "similarity_score": p.get("similarity_score", p.get("final_similarity", 0.0))
            }
            for p in similar_projects 
        ]
        
        # Ø­ÙØ¸ Ø§Ù„Ù…Ø´Ø±ÙÙŠÙ† Ø§Ù„Ù…ÙˆØµÙ‰ Ø¨Ù‡Ù…
        idea_data["recommended_supervisors"] = [
            {
                "_id": str(s.get("_id", "")),
                "name": s.get("Name") or s.get("name", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "email": s.get("Email") or s.get("email", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "department": s.get("Department") or s.get("department", "ØºÙŠØ± Ù…Ø­Ø¯Ø¯"),
                "recent_papers": [
                    {"title": p.get("title", "N/A"), "year": p.get("year", 0)}
                    for p in s.get("recent_papers", [])[:3]
                    if p
                ],
                "research_match_score": float(s.get("reranked_score", 0.0)),
                "justification": s.get("justification", "N/A")
            }
            for s in supervisors_for_prompt[:5]
        ]
        
        result = ideas_collection.insert_one(idea_data)
        print(f"âœ… Saved to MongoDB: {result.inserted_id}\n")
        print(f"{'='*60}\n")
        
        return {
            "id": str(result.inserted_id),
            "message": "âœ… ØªÙ… Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø¨Ù†Ø¬Ø§Ø­" if detected_language == "ar" else "âœ… Analysis completed successfully",
            "detected_language": detected_language,  # ğŸ†• Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…ÙƒØªØ´ÙØ©
            "stage_1_initial_analysis": initial_analysis,
            "stage_2_extended_analysis": extended_analysis,
            "similar_projects": idea_data["similar_projects"], 
            "recommended_supervisors": idea_data["recommended_supervisors"],
            "skills_analysis": {
                "matched_skills": skills_comparison["matched"],
                "gap_skills": skills_comparison["gaps"],
                "match_percentage": skills_comparison["match_percentage"]
            }
        }
        
    except Exception as e:
        print(f"\nâŒ ERROR in analyze_idea:")
        print(f"   {str(e)}")
        traceback.print_exc()
        raise HTTPException(
            status_code=500, 
            detail=f"Analysis failed: {str(e)}"
        )
